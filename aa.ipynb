{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "#from torch_geometric.data import Data\n",
    "#from torch_geometric.loader import DataLoader\n",
    "\n",
    "from data.ag.action_genome import AG, ag_collate_fn\n",
    "\n",
    "from models.rgcn import RGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split: train length: 6388\n",
      "split: test length: 1597\n"
     ]
    }
   ],
   "source": [
    "root = '/data/Datasets/ag/'\n",
    "trainset = AG(root, split='train', subset_file='data/ag/subset_shelve')\n",
    "testset = AG(root, split='test', subset_file='data/ag/subset_shelve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12135\n",
      "2963\n"
     ]
    }
   ],
   "source": [
    "print(len(trainset))\n",
    "print(len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_loader = DataLoader(trainset, batch_size=4, collate_fn=ag_collate_fn)\n",
    "ag_loader = DataLoader(trainset, batch_size=4, collate_fn=ag_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_feature_size = 32\n",
    "num_obj_classes = len(trainset.object_classes)\n",
    "num_verb_classes = len(trainset.verb_classes)\n",
    "num_rel_classes = len(trainset.relationship_classes)\n",
    "model = RGCN(num_obj_classes, node_feature_size, num_verb_classes, num_rel_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('46GP8.mp4/000272.png_92', <PIL.Image.Image image mode=RGB size=480x270 at 0x74B302237700>, Data(x=[1, 36], edge_index=[0], edge_attr=[0, 26], y=[1], node_type=[1], edge_type=[0], w=[1], o=[1], id='46GP8.mp4/000272.png'), 92), ('N11GT.mp4/000205.png_98', <PIL.Image.Image image mode=RGB size=480x360 at 0x74B450577C10>, Data(x=[3, 36], edge_index=[2, 8], edge_attr=[8, 26], y=[1], node_type=[3], edge_type=[8], w=[1], o=[1], id='N11GT.mp4/000205.png'), 98), ('N11GT.mp4/000147.png_153', <PIL.Image.Image image mode=RGB size=480x360 at 0x74B33D156730>, Data(x=[3, 36], edge_index=[2, 8], edge_attr=[8, 26], y=[1], node_type=[3], edge_type=[8], w=[1], id='N11GT.mp4/000147.png'), 153), ('KRF68.mp4/000255.png_148', <PIL.Image.Image image mode=RGB size=480x360 at 0x74B304D40A30>, Data(x=[4, 36], edge_index=[2, 9], edge_attr=[9, 26], y=[1], node_type=[4], edge_type=[9], w=[1], o=[1], id='KRF68.mp4/000255.png'), 148)]\n",
      "('46GP8.mp4/000272.png_92', 'N11GT.mp4/000205.png_98', 'N11GT.mp4/000147.png_153', 'KRF68.mp4/000255.png_148')\n",
      "(<PIL.Image.Image image mode=RGB size=480x270 at 0x74B302237700>, <PIL.Image.Image image mode=RGB size=480x360 at 0x74B450577C10>, <PIL.Image.Image image mode=RGB size=480x360 at 0x74B33D156730>, <PIL.Image.Image image mode=RGB size=480x360 at 0x74B304D40A30>)\n",
      "(Data(x=[1, 36], edge_index=[0], edge_attr=[0, 26], y=[1], node_type=[1], edge_type=[0], w=[1], o=[1], id='46GP8.mp4/000272.png'), Data(x=[3, 36], edge_index=[2, 8], edge_attr=[8, 26], y=[1], node_type=[3], edge_type=[8], w=[1], o=[1], id='N11GT.mp4/000205.png'), Data(x=[3, 36], edge_index=[2, 8], edge_attr=[8, 26], y=[1], node_type=[3], edge_type=[8], w=[1], id='N11GT.mp4/000147.png'), Data(x=[4, 36], edge_index=[2, 9], edge_attr=[9, 26], y=[1], node_type=[4], edge_type=[9], w=[1], o=[1], id='KRF68.mp4/000255.png'))\n",
      "(92, 98, 153, 148)\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "weight = len() / (num_affd_classes * trainset.verb_label_counts)\n",
    "weight = torch.tensor(weight, dtype=torch.float).to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=weight)\n",
    "\n",
    "P = model.parameters()\n",
    "\n",
    "optimizer = torch.optim.Adam(P, lr=1e-3)\n",
    "\n",
    "for e in range(epochs):\n",
    "    for batch in ag_loader:\n",
    "        ids, imgs, sgs, actions = batch\n",
    "        sg_batch = \n",
    "\n",
    "\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ilp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
