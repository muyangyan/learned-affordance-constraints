{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import tensorboard\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "#from torch_geometric.data import Data\n",
    "#from torch_geometric.loader import DataLoader\n",
    "\n",
    "from data.ag.action_genome import AG\n",
    "\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from models.rgcn import RGCN\n",
    "from torchvision.models import vit_b_16, ViT_B_16_Weights, VisionTransformer\n",
    "\n",
    "import pytorch_lightning as L \n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torchmetrics\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#warnings.filterwarnings(\"default\")\n",
    "\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split: train length: 6388\n",
      "split: test length: 1597\n"
     ]
    }
   ],
   "source": [
    "root = '/data/Datasets/ag/'\n",
    "train_set = AG(root, split='train', subset_file='data/ag/subset_shelve')\n",
    "val_set = AG(root, split='test', subset_file='data/ag/subset_shelve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=16, collate_fn=train_set.verb_pred_collate, num_workers=16, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=128, collate_fn=val_set.verb_pred_collate, num_workers=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, num_classes, head=True):\n",
    "        super(ViT, self).__init__()\n",
    "        vit = vit_b_16(weights=ViT_B_16_Weights.DEFAULT)\n",
    "        vit.heads = torch.nn.Identity()\n",
    "        self.head = head\n",
    "\n",
    "        #freeze the backbone\n",
    "        for param in vit.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        #set the head to use our num classes\n",
    "        vit.heads = torch.nn.Linear(vit.hidden_dim, num_classes)\n",
    "        self.vit = vit\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.vit(x)\n",
    "        if self.head:\n",
    "            return F.softmax(x, dim=-1)\n",
    "        return x\n",
    "\n",
    "class JointModel(nn.Module):\n",
    "    def __init__(self, rgcn_params, vit_hidden_dim, num_classes):\n",
    "        super(JointModel, self).__init__()\n",
    "        num_obj_classes, node_feature_size, rgcn_hidden_dim, num_rel_classes = rgcn_params\n",
    "        self.rgcn = RGCN(num_obj_classes, node_feature_size, rgcn_hidden_dim, num_rel_classes, head=False)\n",
    "        self.vit = ViT(vit_hidden_dim, head=False)\n",
    "        self.head = nn.Linear(vit_hidden_dim + rgcn_hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, img, sg):\n",
    "        img = self.vit(img)\n",
    "        sg = self.rgcn(sg)\n",
    "        hidden_state = torch.cat((img, sg), dim=1)\n",
    "        return F.softmax(self.head(hidden_state), dim=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trying pytorch lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying pytorch lightning\n",
    "\n",
    "class JointModelLightning(L.LightningModule):\n",
    "    def __init__(self, model_params, weight, model_type='joint'):\n",
    "        super().__init__()\n",
    "        self.model_type = model_type\n",
    "        rgcn_params, vit_hidden_dim, num_classes = model_params \n",
    "        if model_type == 'joint':\n",
    "            self.model = JointModel(rgcn_params, vit_hidden_dim, num_classes)\n",
    "        elif model_type == 'rgcn':\n",
    "            num_obj_classes, node_feature_size, rgcn_hidden_dim, num_rel_classes = rgcn_params\n",
    "            self.model = RGCN(num_obj_classes, node_feature_size, num_classes, num_rel_classes, head=True)\n",
    "        elif model_type == 'vit':\n",
    "            self.model = ViT(num_classes, head=True)\n",
    "        self.weight = weight\n",
    "        \n",
    "        #epoch metrics\n",
    "        self.train_accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes)\n",
    "        self.val_accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes)\n",
    "                \n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        \n",
    "    def forward(self, img, sg):\n",
    "        if self.model_type == 'rgcn':\n",
    "            return self.model(sg)\n",
    "        elif self.model_type == 'vit':\n",
    "            return self.model(img)\n",
    "        else:\n",
    "            return self.model(img, sg)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        ids, imgs, sgs, verbs, labels = batch\n",
    "        out = self(imgs, sgs)\n",
    "        \n",
    "        loss = F.cross_entropy(out, labels, weight=self.weight)\n",
    "        acc = self.train_accuracy(out, labels)\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        ids, imgs, sgs, verbs, labels = batch\n",
    "        out = self(imgs, sgs)\n",
    "        \n",
    "        val_loss = F.cross_entropy(out, labels, weight=self.weight)\n",
    "        val_acc = self.val_accuracy(out, labels) \n",
    "        \n",
    "        self.log('val_loss', val_loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', val_acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "            \n",
    "        return val_loss\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 33 26\n",
      "tensor([ 1.9013,  0.5761,  4.8325,  3.9094,  1.1152,  0.8528,  3.0791, 12.8866,\n",
      "         0.5155,  1.1297,  5.6119, 57.9899,  0.4525,  5.3529,  1.6890,  4.3492,\n",
      "         0.1672,  2.0835,  0.4018,  0.5585,  0.9829,  2.7397,  0.4349,  0.7801,\n",
      "         8.2843,  0.7872,  0.8507, 10.2335,  2.8995,  1.3331,  2.2162,  0.7419,\n",
      "         3.3137], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "epochs = 10\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "node_feature_size = 32\n",
    "num_obj_classes = len(train_set.object_classes)\n",
    "num_verb_classes = len(train_set.verb_classes)\n",
    "num_rel_classes = len(train_set.relationship_classes)\n",
    "print(num_obj_classes, num_verb_classes, num_rel_classes)\n",
    "\n",
    "rgcn_hidden_dim, vit_hidden_dim = 32, 32\n",
    "rgcn_params = (num_obj_classes, node_feature_size, rgcn_hidden_dim, num_rel_classes)\n",
    "model_params = (rgcn_params, vit_hidden_dim, num_verb_classes)\n",
    "\n",
    "weight = len(train_set) / (num_verb_classes * train_set.verb_label_counts)\n",
    "weight = torch.tensor(weight, dtype=torch.float).to(device)\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4]\n",
      "\n",
      "  | Name           | Type               | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | model          | RGCN               | 30.0 K | train\n",
      "1 | train_accuracy | MulticlassAccuracy | 0      | train\n",
      "2 | val_accuracy   | MulticlassAccuracy | 0      | train\n",
      "--------------------------------------------------------------\n",
      "30.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "30.0 K    Total params\n",
      "0.120     Total estimated model params size (MB)\n",
      "10        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 718/718 [00:42<00:00, 16.77it/s, v_num=11, train_loss_step=3.480, val_loss_step=7.230, val_loss_epoch=3.670, val_acc=0.928, train_loss_epoch=3.450, train_acc=0.913]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 718/718 [00:42<00:00, 16.77it/s, v_num=11, train_loss_step=3.480, val_loss_step=7.230, val_loss_epoch=3.670, val_acc=0.928, train_loss_epoch=3.450, train_acc=0.913]\n"
     ]
    }
   ],
   "source": [
    "model_type = 'rgcn'\n",
    "\n",
    "# Initialize model and trainer\n",
    "lightning_model = JointModelLightning(model_params, weight, model_type='rgcn')\n",
    "\n",
    "# Setup callbacks and logger\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='checkpoints/',\n",
    "    filename='joint-model-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=3,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=f\"{model_type}_model\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator='gpu',\n",
    "    devices=[0],\n",
    "    callbacks=[checkpoint_callback],\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(lightning_model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.data.collate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ilp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
